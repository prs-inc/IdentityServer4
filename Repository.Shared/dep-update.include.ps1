# THIS FILE IS NOT MEANT TO BE USED DIRECTLY 
# it should be included in dependencies-update.ps1 at the root
. ([System.IO.Path]::Combine($PSScriptRoot, 'filesystem.include.ps1'))

# need to make sure PowerShell is using latest security protocol on older OS versions
# to make http requests. The old SSL3 will fail on most https based connections - such 
# as the one to https://aka.ms
[Net.ServicePointManager]::SecurityProtocol = [Net.ServicePointManager]::SecurityProtocol -bor [Net.SecurityProtocolType]::Tls12

# the SilentlyContinue improves performance because it allows the use of the -Name parameter that
# will find the package provider for the majority of cases where it is already installed.
Write-Host "checking to see if NuGet package provider is installed"
if ($null -eq (Get-PackageProvider -Name 'NuGet' -ListAvailable -ErrorAction SilentlyContinue)){
    Install-PackageProvider -Name 'NuGet' -MinimumVersion 2.8.5.201 -Force
}

# This is an MS run repository so pretty sure we can trust it and get rid of the prompts asking
# if we trust it.
if ($null -eq (Get-PSRepository |Where-Object {$_.Name -eq 'PSGallery' -and $_.InstallationPolicy -eq 'Trusted'})) {
    Write-Host "trusting PSGallery to install packages from"
    Set-PSRepository -Name 'PSGallery' -InstallationPolicy Trusted
}

Write-Host "checking to see if Az.Storage module is installed"
if (!(Get-Module -Name 'Az.Storage' -ListAvailable)) {
    Write-Host 'installing Az.Storage module'
    Install-Module -Name 'Az.Storage' -Scope CurrentUser -Confirm:$false -Force
}

# Using the PowerShell version of NVM because it runs on both Windows (full PowerShell)
# and Linux (PowerShell core) and provides a much nicer interface than dealing with
# parsing text output.
Write-Host "checking to see if nvm module is installed"
if (!(Get-Module -Name 'nvm' -ListAvailable)){
    Write-Host 'installing nvm module'
    Install-Module -Name 'nvm' -Scope CurrentUser -Confirm:$false -Force
}

Import-Module -Name 'Az.Storage'
Import-Module -Name 'nvm'

# this local directory will be used if present to save traffic to/from Azure because
# that is much slower than pulling from a local file
$localCachePath = Join-Path ((Get-Location).Drive.Root) ".software-library"
if (!(Test-Path -Path $localCachePath)) {
    Write-Host "did not find $localCachePath on current disk - so using directory in user profile"
    $folderPath = [System.Environment+SpecialFolder]::UserProfile
    $userprofile = [System.Environment]::GetFolderPath($folderPath)
    $localCachePath = Join-Path $userprofile ".software-library"

    if (!(Test-Path -Path $localCachePath)){
        Write-Host "creating $localCachePath on current disk to cache software-library downloads"
        New-Item -Path $localCachePath -ItemType Directory
    }
}

# 
# https://casemaxdata01.blob.core.windows.net/software-library
# ?sv=2018-03-28&si=software-library-read-2020&sr=c&sig=5k%2FNn%2B7am1CIsQ5DkxGqWfinAhwT6sSrWNkW80as1xo%3D
# 
# This can be generated by Microsoft Azure Storage Explorer and should be generated based on an Access policy.  As long
# as an Access Policy is used then the token can be revoked by changing something about the Access Policy.
# This token will expire on 1/31/202# unless the access policy is changed before then to invalidate it
$sasToken = $env:SoftwareLibrarySasToken
Write-Host "using sas token $sasToken that was read from env variable"

# this is the password for the PFX files used for the CMSAuth Token Signing
$pfxPwd = 'c4Wk-NkLji09OYA4^l4G7bj^OSEsQ6Fx' | ConvertTo-SecureString -Force -AsPlainText

# This is the name of the storage account in the Azure Portal
$acctName = 'casemaxdata01' 

# what azure context should be used for performing blob operations
[Microsoft.WindowsAzure.Commands.Storage.AzureStorageContext]$storageCtx

# [Microsoft.WindowsAzure.Commands.Common.Storage.ResourceModel.AzureStorageBlob] 
# these are all the blobs in the container
$blobInfos = $null

# This will be set if we are pulling down from Azure because we will get the current AzCopy.exe
# by downloading from MS and then extracting.  The path with vary depending on the version downloaded.
$azcopyPath = ''

$container = 'software-library'

# make sure local .external-bin exists
if (!(Test-Path -Path '.external-bin')) { 
    New-Item -Path '.external-bin' -ItemType directory 
}

# Need to get the list of files that have been downloaded and are the
# local directories that were extracted.  This is being done to make it
# easier to switch between branches that have different dependencies
$filesXmlPath = (Combine-Paths -Paths '.external-bin', '_files.xml')
if (!(Test-Path -Path $filesXmlPath)){
    New-Item -Path $filesXmlPath
    Set-Content -Path $filesXmlPath -Value '<files/>'
}

# need to use the Resolve-Path because need the full path for XmlDocument
# operations since .net interop code will be running in a different directory
$xmlFilePath = (Resolve-Path -Path $filesXmlPath)
$doc = [xml](Get-Content -Path $xmlFilePath)

Write-Host 'initializing connection to Azure Storage'

#This will create a context to the Url 'https://casemaxdata01.blob.core.windows.net/'
$storageCtx = New-AzStorageContext -SasToken $sasToken -StorageAccountName $acctName 

# get all of the blobs in one shot and later we will filter to find the correct one
$blobInfos = Get-AzStorageBlob -Container $container -Context $storageCtx

function Test-CommandExists {
    param (
        [String]$Name
    )
    $cmd = Get-Command -Name $Name -ErrorAction 'SilentlyContinue'
    return ($null -ne $cmd)
}

function Add-NuGetPackageSource {
    param(
        [string]$Name,
        [string]$Path
    )

    # need to use the Resolve-Path because need the full path for XmlDocument
    # operations since .net interop code will be running in a different directory
    if (!(Test-Path -Path 'NuGet.config')){
        Set-Content -Path 'NuGet.config' `
            -Value '<?xml version="1.0" encoding="utf-8"?><configuration><packageSources /></configuration>' `
            -Encoding 'utf8'
    }
    
    $configFilePath = (Resolve-Path -Path 'NuGet.config')
    $doc = [xml](Get-Content -Path $configFilePath)

    # want to get rid of the current directory if the $Version that was downloaded is different
    # than the version that is on disk.  
    $sourcesEl = $doc.SelectSingleNode("//configuration/packageSources")
    $el = $sourcesEl.SelectSingleNode("add[@key='$Name']")
    if ($el -eq $null){
        Write-Host "did not find file node with attribute product containing value $Product"
        $el = $doc.CreateElement("add")
        $el.SetAttribute("key", $Name)
    }
    
    $el.SetAttribute("value", $Path)
    $sourcesEl.AppendChild($el)
    $doc.Save("$configFilePath")
}

function Download-AzCopy {
    # Downloading AzCopy from MS website and extract it locally.  Doing it this way so don't have to
    # worry about installing software on build servers or developer workstations.  Using AzCopy
    # instead of Blob APIs because it is more effecient when working with large files
    $lastCheckedAt = [System.DateTime]::MinValue

    $azCopyInfoPath = Combine-Paths -Paths $localCachePath, "AzCopy-info.txt"
    if (Test-Path -Path $azCopyInfoPath) {
        Write-Host "reading last time AzCopy was checked for new version"
        $lastCheckedAt = [System.DateTime]::Parse((Get-Content -Path $azCopyInfoPath))
    }

    if ($lastCheckedAt.AddDays(7) -gt [System.DateTime]::Now){
        Write-Host "AzCopy was last checked at $lastCheckedAt - that falls within the last week - not downloading again"
    }
    else {
        if ($IsLinux){
            Write-Host "downloading AzCopy v10 for Linux into local cache"
            Invoke-WebRequest -Uri "https://aka.ms/downloadazcopy-v10-linux" `
                -OutFile "$localCachePath/AzCopy-10.0.tar" `
                -UseBasicParsing
        }
        else {
            Write-Host "downloading AzCopy v10 for Windows into local cache"
            Invoke-WebRequest -Uri "https://aka.ms/downloadazcopy-v10-windows" `
                -OutFile (Combine-Paths -Paths $localCachePath, 'AzCopy-10.0.zip') `
                -UseBasicParsing
        }

        Write-Host "updating AzCopy-info.txt"
        Set-Content -Path $azCopyInfoPath -Value (Get-Date)
    }

    # tar needs to have the directory present to extract it to a specific dir
    $azCopyDirPath = Combine-Paths -Paths '.external-bin', 'AzCopy'
    if (!(Test-Path -Path $azCopyDirPath)){
        Write-Host "creating $azCopyDirPath to extract downloaded AzCopy into"
        New-Item -Path $azCopyDirPath -ItemType Directory | Out-Null
    }

    if ($IsLinux){
        Write-Host "extracting downloaded AzCopy v10 tar from local cache into .external-bin"
        # tar will write out the files it extracts and it will be interpreted as Write-Ouptut
        # and mess up the path to $azCopyExe that is returned
        tar -C ./.external-bin/AzCopy/ -xvf $localCachePath/AzCopy-10.0.tar | Out-Null
        
        $azcopySearchPath = "./.external-bin/AzCopy/azcopy_linux*/azcopy"
    }
    else {
        Write-Host "extracting downloaded AzCopy v10 zip from local cache into .external-bin"
        Expand-Archive -Path (Combine-Paths -Paths $localCachePath, 'AzCopy-10.0.zip') `
                -DestinationPath $azCopyDirPath `
                -Force
        
        $azcopySearchPath = Combine-Paths -Paths '.external-bin', 'AzCopy', 'azcopy_windows*', 'azcopy.exe'
    }

    $azcopyExe = (Get-ChildItem -Path "$azcopySearchPath" | 
        Sort-Object -Property LastWriteTime |
        Select-Object -Last 1).FullName

    Write-Output $azcopyExe
}

function Download-NuGet {
    param (
        [string]$Version = 'latest'
    )

    $downloadPath = Combine-Paths -Paths $localCachePath, "nuget-$Version.exe"

    if ($Version -eq 'latest'){
        Write-Host "downloading latest nuget.exe version"

        Invoke-WebRequest -Uri "https://dist.nuget.org/win-x86-commandline/latest/nuget.exe" `
            -OutFile $downloadPath
    }
    else {
        Write-Host "checking to see if nuget $Version is already downloaded"

        if (!(Test-Path -Path $downloadPath)){
            Write-Host "downloading nuget.exe $Version"
            Invoke-WebRequest -Uri "https://dist.nuget.org/win-x86-commandline/v$Version/nuget.exe" `
                -OutFile $downloadPath
        }
        else {
            Write-Host "nuget $Version already downloaded at $downloadPath"
        }
    }

    $nugetPath = (Join-Path -Path ".external-bin" -ChildPath "NuGet")
    $nugetExe = (Join-Path -Path $nugetPath -ChildPath "nuget.exe")

    if (!(Test-Path -Path $nugetPath)){
        New-Item -Path $nugetPath -ItemType Directory -Force | Out-Null
    }

    # copy the downloaded version to .external-bin
    Copy-Item -Path $downloadPath -Destination $nugetExe

    # now that it has been downloaded convert it to the full path
    $nugetExe = (Resolve-Path -Path $nugetExe)

    if ($IsLinux -eq $true){
        $nugetSh = (Join-Path -Path $nugetPath -ChildPath "nuget")
        Write-Host "creating nuget shell file for exe $nugetExe"

        # create a shell script file to call the exe
        Set-Content -Path "$nugetSh" `
            -Value "#!/bin/sh", "exec mono $nugetExe $@ &", "wait" `
            -Encoding ascii

        # make it executable
        bash -c "chmod +x $nugetSh"

        $nugetExe = (Resolve-Path -Path $nugetSh)
    }
    
    Write-Output $nugetExe
}

function Download-CaseMaxFile {
    param (
        [string]$BuildType = 'dev',
        [string]$Branch = '',
        [Switch]$PackagesOnly = $false
    )

    # if there is no branch specified then assume we are pulling down the same
    # branch of the current repository
    if ($Branch -eq '') {
        $Branch = $(git.exe symbolic-ref --short HEAD)
    }

    $version = "$branch-$BuildType"
    
    # dev builds are not copied to the software-library so need to grab
    # the ccnet build instead
    if ($BuildType -eq "dev"){
        $version = "$branch-ccnet"
    }

    # nightly builds are no longer done for CaseMax, they are only done for
    # Firms so need to grab the last ccnet build instead
    if ($BuildType -eq "nightly"){
        $version = "$branch-ccnet"
    }

    # This is getting the binaries so CaseMax can be run from an .external-bin
    # some codebases only need to get the NuGet packages and don't care about
    # all of the binaries to run the rest of CaseMax.
    if ($PackagesOnly -eq $false){
        Download-File -Product "CMS" -Version $version -Extension ".zip"
    }
    
    # This is getting the NuGet Packages so CaseMax assemblies can be referenced
    # in .csproj files
    Download-File -Product "CMS-packages" -Version "$version" -Extension ".zip"
    
}

function Get-CaseMaxPackageVersion {
    Get-PackageVersion -Path (Combine-Paths -Paths '.external-bin', 'CMS-packages') `
        -Name 'PRS.CMS.Domain'
}

function Get-PackageVersion {
    param (
        [string]$Path,
        [string]$Name
    )

    $nupkgFile = @(Get-ChildItem -Path $Path -Filter "$Name.*.nupkg" |
            Sort-Object -Property "LastWriteTime" -Descending)[0]

    Write-Host "the nupkgFile is $nupkgFile"
    $versionParts = $nupkgFile.Name.Replace("$Name.", "").Replace(".nupkg", "").Split('-')

    $ver = $versionParts[0]

    if ($versionParts.Length -eq 1){
        Write-Output $ver
    }
    else {
        Write-Output "$ver-alpha*"
    }
}

function Download-BakFile {
    param (
        [string]$Product,
        [string]$Version,
        [string]$Extension = ".7z"
    )

    # Need to have it extract to the bak directory because manage-databases.build.ps1 assumes all bak
    # files are in the bak directory.  So when a database name is passed to it the script knows where
    # to look for bak files.
    Download-File -Product "bak-$Product" -Version "$Version" -Extension $Extension -ExtractDirName "bak"
}

function Setup-Nodejs {
    param (
        [string]$Version
    )
    
    # if the profile file doesn't exist then the NVM module seems to have trouble
    if (!(Test-Path -Path $PROFILE)){
        New-Item -Path $PROFILE -ItemType File -Force
    }

    $v = (Get-NodeVersions -Filter "$Version")

    if ($v -eq $null) {
        Write-Host "installing Node $Version"
        Install-NodeVersion -Version $Version
        $v = $Version
    }
    else {
        Write-Host "The Node $Version was found with semver $v"
    }

    Write-Host "switching Node to $Version"
    Set-NodeVersion -Version $Version -Persist User

    # this is being done because node-sass invokes node when npm is installing it.
    # https://github.com/nodenv/nodenv/wiki/FAQ#npm-warning-about-mismatched-binaries
    if ($IsWindows -ne $false) {
        Write-Host 'node-sass npm config'
        & npm config set scripts-prepend-node-path auto 2>&1
    }
}

function Download-File {
    param (
        [string]$Product, 
        [string]$Version,
        [string]$Extension = ".7z",
        [string]$ExtractDirName = "$Product"
    )
    $filename = "$Product-$Version$Extension"
    $localCacheFilePath = (Join-Path -Path "$localCachePath" -ChildPath "$filename")
    $localCacheStatus = ''
  
    # it is possible this file doesn't exist on the Azure blob (such as a CMS nightly|rc build)
    $blobInfo = $blobInfos | Where-Object {$_.Name -eq $filename } | Select-Object -First 1

    # only attempt to pull down the file from the local cache directory if
    # the path can be found.
    if (Test-Path $localCacheFilePath) {
    
        $localCacheFile = Get-Item -Path $localCacheFilePath
        $localCacheFilesize = $localCacheFile.Length
        $localCacheModified = $localCacheFile.LastWriteTime
        Write-Host "The local cache file $localCacheFilePath is $localCacheFilesize bytes and was last modified at $localCacheModified"

        # if different file size
        if (($null -ne $blobInfo) -and ($localCacheFilesize -ne $blobInfo.Length)){
            Write-Host "will need to pull blob down $filename from Azure because the local cache has a different FileSize on it"
            $localCacheStatus = 'old'

            # by removing the item from disk the if block below that checks to see if the source is LocalCache
            # and the path is missing will see that and will copy the file out
            Write-Host "deleted the local cache file $localCacheFilePath because it needs to be replaced"
            Remove-Item -Path $localCacheFilePath
        }
    }
    else {
        $localCacheStatus = 'missing'
    }

    # check if the file in the local cache should be considered 'old' because it does not match the file size of what is
    # on the Azure portal
    if ($localCacheStatus -in 'old', 'missing') {
        $filesize = 0
        $modified = [System.DateTime]::Parse('1/1/2000')

        if ($localCacheStatus -eq 'old') {
            $filesize = $localCacheFile.Length
            $modified = $localCacheFile.LastWriteTime
            Write-Host "The local cache file $localCacheFilePath is $filesize bytes and was last modified at $modified"
        }
        
        $blobSize = $blobInfo.Length
        $blobModified = $blobInfo.LastModified.LocalDateTime
        
        # if different file size
        if ($filesize -ne $blobSize){
            $blobUrl = $storageCtx.BlobEndPoint + $container + '/' + $blobInfo.Name + $sasToken

            Write-Host "pulling blob down $filename from Azure that is $blobSize bytes and was last modified at $blobModified at url $blobUrl"

            & $azcopyPath copy $blobUrl $localCacheFilePath
            
            # need to set the write time to be same time as the blob has otherwise it will continuously pull
            # down the same blob.  Actually - that check has been removed but I still want it to have the same
            # ModifiedDate as the Blob in Azure just for easy visual compare and so I can see if it has been downloaded
            if ($IsWindows) {
                (Get-Item -Path $localCacheFilePath).LastWriteTime = $blobModified
            }
        }
        else {
            Write-Host 'local file matches size and modified date with Azure'
        }
    }

    Extract-DownloadedFile -Product $Product `
        -Version $Version `
        -ZipFilePath $localCacheFilePath `
        -ExtractDirName $ExtractDirName

}

function Download-ChromeDriver {
    # get chrome version
    $installPaths = @(
        "C:\Program Files\Google\Chrome\Application\chrome.exe", 
        "C:\Program Files (x86)\Google\Chrome\Application\chrome.exe"
    )

    $chromeInstallPath = $installPaths | Where-Object {Test-Path -Path $_} | Select-Object -First 1
    if ($chromeInstallPath -eq $null){
        Write-Host "Google Chrome was not found in x64 or x86 install location"
        return;
    }
    
    $file = (Get-ChildItem -Path $chromeInstallPath)
    $fileVersion = [System.Version]::Parse($file.VersionInfo.FileVersion)

    # use the instructions at https://chromedriver.chromium.org/downloads/version-selection
    # to determine the driver version to download
    $latestBuildVer = $fileVersion.ToString(3)

    Write-Host "discovering latest Web Driver for Chrome build $latestBuildVer at $chromeInstallPath"
    $driverVer = (Invoke-WebRequest -Uri "https://chromedriver.storage.googleapis.com/LATEST_RELEASE_$latestBuildVer" -UseBasicParsing).Content

    # look in local cache path to see if Web driver version already exists
    # if we don't find it then we need to download it
    $localZipPath = Join-Path -Path $localCachePath -ChildPath "chromedriver_win32-$driverVer.zip"
    
    if (!(Test-Path -Path $localZipPath)){
        Write-Host "downloading Web Driver $driverVer for Chrome $fileVersion"
        Invoke-WebRequest -Uri "https://chromedriver.storage.googleapis.com/$driverVer/chromedriver_win32.zip" `
            -OutFile $localZipPath `
            -UseBasicParsing
        Write-Host "downloaded Web Driver $driverVer from chromedriver.storage.googleapis.com"
    }
    else {
        Write-Host "The Web Driver $driverVer for Chrome $fileVersion was already downloaded"
    }

    Extract-DownloadedFile -Product 'chromedriver_win32' `
            -Version $driverVer `
            -ZipFilePath $localZipPath `
            -ExtractDirName 'chromedriver_win32'
}

function Download-EdgeDriver {

    # get edge version
    $edgeInstallPath = "C:\Program Files (x86)\Microsoft\Edge\Application\msedge.exe"
    if (!(Test-Path -Path $edgeInstallPath)){
        Write-Host "Microsoft Edge was not found at path $edgeInstallPath"
        return;
    }
    
    $file = (Get-ChildItem -Path $edgeInstallPath)
    $fileVersion = [System.Version]::Parse($file.VersionInfo.FileVersion)

    # MS Edge releases a web driver that is the same version as the browser so don't 
    # need to discover the web driver version like we do with Chrome
    $driverVer = $fileVersion.ToString()

    # look in local cache path to see if web driver version already exists
    # if we don't find it then we need to download it
    $localZipPath = Join-Path -Path $localCachePath -ChildPath "edgedriver_win64-$driverVer.zip"
    
    if (!(Test-Path -Path $localZipPath)){
        Write-Host "downloading Web Driver $driverVer for Edge $fileVersion"
        
        Invoke-WebRequest -Uri "https://msedgedriver.azureedge.net/$driverVer/edgedriver_win64.zip" `
            -OutFile $localZipPath `
            -UseBasicParsing

        Write-Host "downloaded Web Driver $driverVer from msedgedriver.azureedge.net"
    }
    else {
        Write-Host "The Web Driver $driverVer for Edge $fileVersion was already downloaded"
    }

    Extract-DownloadedFile -Product 'edgedriver_win64' `
            -Version $driverVer `
            -ZipFilePath $localZipPath `
            -ExtractDirName 'edgedriver_win64'
}

function Extract-DownloadedFile {
    param (
        [string]$Product,
        [string]$Version,
        [string]$ZipFilePath,
        [string]$ExtractDirName
    )

    # want to get rid of the current directory if the $Version that was downloaded is different
    # than the version that is on disk.  
    $el = $doc.SelectSingleNode("//files/file[@product='$Product']")
    if ($el -eq $null){
        Write-Host "did not find file node with attribute product containing value $Product"
        $el = $doc.CreateElement("file")
        $el.SetAttribute("product", $Product)
        $el.SetAttribute("version", "0")
        $doc.DocumentElement.AppendChild($el)
        $doc.Save("$xmlFilePath")
    }

    if ($el.version -ne $Version){

        $p = Join-Path -Path ".external-bin" -ChildPath "$Product"
        if (Test-Path -Path $p) {
            Write-Host "cleaning the path for new version of $Product"
            # don't know why I have to fall back to using CMD.EXE, but
            # that is the game we play with PowerShell - find out why a seemingly obvious thing
            # should just work only to find out there is problem with a -Item command.
            if ($IsLinux -eq $true){
                rm -r "$p"
            }
            else {
                cmd.exe /c rd /s /q "$p"
            }
        }

        Write-Host "writing $Version of $Product to _files.xml"
        $el.version = $Version
        $doc.Save("$xmlFilePath")
    }

    Write-Host "extracting $ZipFilePath into $ExtractDirName directory"

    # the files still have to exists in the .external-bin directory
    # otherwise there will be all sorts of problems and basically
    # the build won't work
    # x : extract command
    # -y : assume Yes on all queries
    # -aoa : overwrite all existing files without prompt
    # -o:  output directory
    $zipOutputPath = (Combine-Paths -Paths '.external-bin', $ExtractDirName)
    $zipArgs = @(
        "x",
        "-y",
        "-aoa",
        "-o$zipOutputPath",
        "$ZipFilePath"
    )
    
    if ($IsLinux){
        7z $zipArgs
    }
    else {
        & "C:\Program Files\7-Zip\7z.exe" $zipArgs
    }

}

function Import-CMSAuthCert {
    param (
        [string]$WebServerType,
        [string]$PfxPath
    )

    $cert = Import-PfxCertificate -FilePath $PfxPath -CertStoreLocation Cert:\LocalMachine\My -Password $pfxPwd

    $rsaFile = $cert.PrivateKey.CspKeyContainerInfo.UniqueKeyContainerName
    $path = 'C:\ProgramData\Microsoft\Crypto\RsA\MachineKeys\' + $rsaFile

    $acl = Get-Acl -Path $path

    # When running inside of IIS on dev/build this is the account it will be using
    if ($WebServerType -eq 'iis') {
        $accRule = New-Object System.Security.AccessControl.FileSystemAccessRule 'IIS APPPOOL\CMSAuthAppPool', 'Read,FullControl', 'Allow'
        $acl.AddAccessRule($accRule)
    }

    # When running inside of IIS Express on dev this is the account it will be using.
    # Making the assumption the workstation is in a domain.  Typically the build scripts
    # are executed in shell with 'Run As Administrator' so this requires IIS Express to
    # be launched with same credentials - that is not typically what we want to do.  We
    # want to run IIS Express under a non-elevated user account
    $accRule = New-Object System.Security.AccessControl.FileSystemAccessRule "$env:USERDOMAIN\$env:USERNAME", 'Read,FullControl', 'Allow'
    $acl.AddAccessRule($accRule)
    
    # All any member of the Users group to access these keys
    $accRule = New-Object System.Security.AccessControl.FileSystemAccessRule "Users", 'Read,FullControl', 'Allow'
    $acl.AddAccessRule($accRule)

    Set-Acl -Path $path -AclObject $acl
}

function Import-CMSAuthCerts {
    param (
        $WebServerType
    )

    $path = Combine-Paths -Paths '.external-bin', 'cmsauth-certs'

    if (Test-CommandExists -Name 'Import-PfxCertificate'){
    
        Get-ChildItem -Path (Combine-Paths -Paths $path, '*.pfx') |
            Sort-Object -Property FullName |
            ForEach-Object -Process {
                $pfxPath = $_.FullName
                Write-Host "importing $pfxPath"
                Import-CMSAuthCert -WebServerType $WebServerType -PfxPath $pfxPath
            }

    }
    else {
        Write-Warning "The command Import-PfxCertificate does not exist, the certs must be manually imported"
    }
}

function Sign-Assembly {
    param (
        [string]$Path,
        [string]$AssemblyName,
        [string]$OutputPath
    )
    <#
     # This was being used to sign SecurityDriven.Inferno but I put a zip file out in software-library
     # so I didn't have to worry about path to SDKs.  But leaving this in here to show how it can be
     # done.
     # there is no strong name of this version of the NuGet package so need to sign it ourselves
        .external-bin\NuGet\nuget.exe install Inferno -Version 1.6.2 -OutputDirectory '.external-bin\Inferno\nuget'

        Sign-Assembly -Path ".external-bin\Inferno\nuget\Inferno.1.6.2\lib\net462" `
            -AssemblyName "SecurityDriven.Inferno" `
            -OutputPath ".external-bin\Inferno\net472"

        Sign-Assembly -Path ".external-bin\Inferno\nuget\Inferno.1.6.2\lib\netstandard2.0" `
            -AssemblyName "SecurityDriven.Inferno" `
            -OutputPath ".external-bin\Inferno\netstandard2.0"
    
     #>

    $toolsPath = "C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.7.2 Tools"

    # convert the DLL over to IL
    & "$toolsPath\ildasm.exe" "$Path\$AssemblyName.dll" /out="$Path\$AssemblyName.il"
    
    if (!(Test-Path -Path $OutputPath)) {
        New-Item -Path "$OutputPath" -ItemType Directory
    }
    
    # compile the IL into a DLL and sign it
    & "C:\Windows\Microsoft.NET\Framework\v4.0.30319\ilasm.exe" "$Path\$AssemblyName.il" `
            /res:"$Path\$AssemblyName.res" `
            /dll `
            /key:PRS.CMS.snk `
            /out:"$OutputPath\$AssemblyName.dll"
            
    # verify the strong name in the assembly
    & "$toolsPath\sn.exe" -vf "$OutputPath\$AssemblyName.dll"
    
    Copy-Item -Path "$Path\*" -Include '*.xml', '*.pdb' -Destination $OutputPath
    Remove-Item -Path "$Path\$AssemblyName.il"
    Remove-Item -Path "$Path\$AssemblyName.res"

}

function Update-DirectoryBuildProperty {
    param (
        [string]$Path = 'Directory.Build.props',
        [string]$Name,
        [string]$Value
    )
    
    $propsPath = (Resolve-Path -Path $Path)
    $doc = [xml](Get-Content -Path $propsPath)

    # want to get rid of the current directory if the $Version that was downloaded is different
    # than the version that is on disk.  
    $el = $doc.SelectSingleNode("//PropertyGroup/$Name")
    if ($el -ne $null){
        $el.InnerText = "$Value"
        $doc.Save("$propsPath")
    }
}

function Stop-ExternalBinProcess {
    $path = (Resolve-Path -Path '.external-bin') 
    Get-Process | 
        Where-Object { ($null -ne $_.Path) -and ($_.Path.StartsWith($path)) } |
        Stop-Process -Force -Confirm:$false
}

function Create-SqlServerDockerContainer {
    param (
        [string]$ContainerName,
        [string]$ServerVersion,
        [string]$AdminPassword
    )

    $missing = $null -eq (docker container list --all --filter name="$ContainerName" --format '{{.Names}}')

    if ($missing -eq $true) {
        Write-Host "creating container $ContainerName for SQL Server $ServerVersion"
        docker container create `
           --env "ACCEPT_EULA=Y" `
           --env "SA_PASSWORD=$AdminPassword" `
           --env "MSSQL_BACKUP_DIR=/var/opt/mssql/backup" `
           --publish 1433:1433 `
           --name $ContainerName `
           --hostname $ContainerName `
            "mcr.microsoft.com/mssql/server:$ServerVersion"
    }

    $status = (docker container inspect --format '{{.State.Status}}' "$ContainerName")
    Write-Host "status of container $ContainerName is $status"
    if ($status -ne 'running') {
        Write-Host "starting container $ContainerName"
        docker container start "$ContainerName"
    }
}

function Copy-BakFilesToContainer {
    param (
        [String]
        $ContainerName
    )

    # make sure the backup directory exists in the container
    docker exec "$ContainerName" mkdir -p /var/opt/mssql/backup

    # copy the files in the bak folder over to the container
    docker cp './.external-bin/bak/.' "$($ContainerName):/var/opt/mssql/backup"
}

$azcopyPath = (Download-AzCopy)